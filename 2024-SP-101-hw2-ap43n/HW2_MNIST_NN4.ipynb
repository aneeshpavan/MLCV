{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import from mlcblab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlcvlab.models.nn4 import NN4\n",
    "from mlcvlab.nn.activations import relu, sigmoid, sigmoid_grad, relu_grad\n",
    "from mlcvlab.nn.basis import linear, linear_grad\n",
    "from mlcvlab.nn.batchnorm import BatchNorm\n",
    "from mlcvlab.nn.dropout import dropout, dropout_grad\n",
    "from mlcvlab.nn.losses import l2, l2_grad, cross_entropy, cross_entropy_grad\n",
    "from mlcvlab.optim.adam import Adam\n",
    "from mlcvlab.optim.sgd import SGD\n",
    "from mlcvlab.optim.sync_sgd import sync_sgd\n",
    "# TODO: Import all the necessary code from mlcvlab package as you need... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "    y = y.astype(np.int64)\n",
    "    return x,y\n",
    "\n",
    "def prepare_data(x, y):\n",
    "    y = (y % 2 == 0)\n",
    "    return x, y\n",
    "\n",
    "def split_train_test(x,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=10000, random_state=25)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def minibatch(X_train,y_train,K):\n",
    "    #TODO\n",
    "    # Batch Size: K\n",
    "    # X_train_batches, y_train_batches should be a list of lists of size K.\n",
    "    batches = X_train.shape[0] // K\n",
    "    X_train_batches = np.array_split(X_train, batches)\n",
    "    y_train_batches = np.array_split(y_train, batches)\n",
    "    return X_train_batches, y_train_batches\n",
    "\n",
    "def initialize_model():\n",
    "    #TODO (Can use the similar approach used in HW1)\n",
    "    # e.g. He Initialization for W0-W2, Xavier Initialization for W3\n",
    "    # Also, initialize your model with a dropout parameter of 0.25 and use_batchnorm being true.\n",
    "    W0 = np.random.randn(785, 500) * np.sqrt(2/785)\n",
    "    W1 = np.random.randn(500, 100) * np.sqrt(2/500)\n",
    "    W2 = np.random.randn(100, 50) * np.sqrt(2/100)\n",
    "    W3 = np.random.randn(50, 1) * np.sqrt(2/50)\n",
    "    print(f\"Size of W0 : {W0.shape}, Size of W1 : {W1.shape}, Size of W2 : {W2.shape}, Size of W3 : {W3.shape}\")\n",
    "    four_layer_nn  = NN4(True, 0.25)\n",
    "    four_layer_nn.layers[0].W = W0\n",
    "    four_layer_nn.layers[1].W = W1\n",
    "    four_layer_nn.layers[2].W = W2\n",
    "    four_layer_nn.layers[3].W = W3\n",
    "\n",
    "    return four_layer_nn\n",
    "\n",
    "def train_model(model, X_train_batches, y_train_batches):\n",
    "    #TODO : Call async_SGD and sync_SGD to train two versions of the same model. Compare their outcomes and runtime.\n",
    "    #Update both your models with final updated weights and return them\n",
    "    model_async = sync_sgd(model, X_train_batches, y_train_batches, lr=0.01, mode='train')\n",
    "    return model_async\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    accuracy = None\n",
    "    #TODO: Call model.nn4 to test model.\n",
    "    total = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_sample_test = np.append(X_test[i, :], 1)\n",
    "        y_actual = y_test[i]\n",
    "        y_hat = model.nn4(X_sample_test, mode='test')\n",
    "\n",
    "        predicted_class = 1 if y_hat >= 0.5 else 0\n",
    "\n",
    "        if predicted_class == y_actual:\n",
    "            total += 1\n",
    "\n",
    "    accuracy = total / len(X_test)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anees\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of W0 : (785, 500), Size of W1 : (500, 100), Size of W2 : (100, 50), Size of W3 : (50, 1)\n",
      "Starting SGD...\n",
      "Iteration 1 completed with loss: 0.287700\n",
      "Iteration 2 completed with loss: 0.325896\n",
      "Iteration 3 completed with loss: 0.344710\n",
      "Iteration 4 completed with loss: 0.299171\n",
      "Iteration 5 completed with loss: 0.329399\n",
      "Iteration 6 completed with loss: 0.279759\n",
      "Iteration 7 completed with loss: 0.305589\n",
      "Iteration 8 completed with loss: 0.279359\n",
      "Iteration 9 completed with loss: 0.296616\n",
      "Iteration 10 completed with loss: 0.288702\n",
      "Completed training model - final W : [array([[ 2.50718687e-02, -6.97895234e-03,  3.26923682e-02, ...,\n",
      "        -9.60742976e-03, -4.41972223e-02, -6.97974339e-02],\n",
      "       [ 4.67492253e-02,  9.63786575e-02, -7.05933228e-02, ...,\n",
      "         3.23468296e-02, -2.88305145e-02,  2.89013716e-02],\n",
      "       [ 7.06330905e-02,  4.66712980e-02,  3.00986955e-03, ...,\n",
      "         6.09928250e-02,  5.16900132e-02,  2.99080623e-02],\n",
      "       ...,\n",
      "       [ 4.11018115e-02,  1.30872926e-02,  1.72047616e-02, ...,\n",
      "        -2.82300575e-02,  1.27323164e-01, -1.46392621e-02],\n",
      "       [-1.46940091e-01,  1.46246188e-04,  3.71805826e-02, ...,\n",
      "        -5.99342105e-02, -1.38804770e-02, -7.12027001e-02],\n",
      "       [ 1.27958322e-02,  1.69317132e-02,  3.50279848e-02, ...,\n",
      "         1.08688714e-01,  4.76831237e-03, -2.52520693e-02]]), array([[-0.06199566,  0.07239133, -0.02309674, ...,  0.08439826,\n",
      "        -0.03465254, -0.08994018],\n",
      "       [ 0.03734813, -0.04876958,  0.00291695, ...,  0.10584474,\n",
      "         0.00906621,  0.03176706],\n",
      "       [ 0.09113651,  0.02562483,  0.07143974, ..., -0.12922918,\n",
      "        -0.05033489,  0.00168949],\n",
      "       ...,\n",
      "       [-0.06273633, -0.07638427, -0.05108518, ..., -0.0784523 ,\n",
      "         0.04701271, -0.02144533],\n",
      "       [ 0.12813081, -0.02202002,  0.00163431, ..., -0.03869438,\n",
      "         0.00329289,  0.04716219],\n",
      "       [-0.02367181, -0.0320889 , -0.02389327, ..., -0.12656521,\n",
      "        -0.00443727, -0.03385067]]), array([[-0.16223351,  0.34261736,  0.14196046, ..., -0.18276937,\n",
      "         0.21974189,  0.15841552],\n",
      "       [ 0.0981977 ,  0.31038943, -0.20835191, ...,  0.19387768,\n",
      "         0.07634128,  0.00646494],\n",
      "       [ 0.05299553,  0.42118954,  0.09462099, ..., -0.21598822,\n",
      "         0.20637661, -0.09505397],\n",
      "       ...,\n",
      "       [-0.154783  ,  0.19139984, -0.17540044, ..., -0.09512027,\n",
      "        -0.16367323,  0.11934425],\n",
      "       [-0.1730195 ,  0.02900199,  0.17461728, ..., -0.06475105,\n",
      "         0.01958109, -0.16005642],\n",
      "       [ 0.16648443, -0.06343938,  0.00131764, ..., -0.02699199,\n",
      "         0.06993633,  0.02012416]]), array([[-0.09303071],\n",
      "       [-0.20116087],\n",
      "       [-0.29781451],\n",
      "       [ 0.01866611],\n",
      "       [-0.21767507],\n",
      "       [-0.09652436],\n",
      "       [ 0.10346204],\n",
      "       [ 0.23582712],\n",
      "       [-0.11277325],\n",
      "       [-0.17839583],\n",
      "       [ 0.41582578],\n",
      "       [ 0.04667831],\n",
      "       [ 0.24118952],\n",
      "       [-0.49729069],\n",
      "       [ 0.0751388 ],\n",
      "       [ 0.1287463 ],\n",
      "       [-0.0590085 ],\n",
      "       [-0.23836068],\n",
      "       [ 0.18568939],\n",
      "       [-0.02153764],\n",
      "       [ 0.36810549],\n",
      "       [-0.19444262],\n",
      "       [ 0.28417076],\n",
      "       [-0.16352773],\n",
      "       [ 0.07443067],\n",
      "       [-0.00173972],\n",
      "       [-0.07065042],\n",
      "       [ 0.27495929],\n",
      "       [-0.00644821],\n",
      "       [ 0.28110589],\n",
      "       [-0.13203369],\n",
      "       [-0.22927189],\n",
      "       [ 0.04210566],\n",
      "       [ 0.21480238],\n",
      "       [ 0.10012517],\n",
      "       [-0.2330644 ],\n",
      "       [ 0.04077652],\n",
      "       [ 0.02303324],\n",
      "       [ 0.15811573],\n",
      "       [-0.21711779],\n",
      "       [ 0.22260987],\n",
      "       [-0.11380281],\n",
      "       [-0.08113245],\n",
      "       [-0.02387483],\n",
      "       [ 0.23165793],\n",
      "       [ 0.11135459],\n",
      "       [ 0.24242811],\n",
      "       [-0.00973436],\n",
      "       [ 0.12537556],\n",
      "       [-0.06645607]])]\n",
      "Completed testing model - Accuracy : 0.6689\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#load data\n",
    "x, y = load_dataset()\n",
    "\n",
    "#prepare data\n",
    "x, y = prepare_data(x,y)\n",
    "\n",
    "# split data set\n",
    "X_train, X_test, y_train, y_test = split_train_test(x,y)\n",
    "\n",
    "#initialize model\n",
    "model = initialize_model()\n",
    "\n",
    "K = 100\n",
    "X_train_batches, y_train_batches = minibatch(X_train,y_train,K)\n",
    "\n",
    "#training model\n",
    "model_async = train_model(model, X_train_batches, y_train_batches)\n",
    "\n",
    "#testing model\n",
    "accuracy = test_model(model_async, X_test, y_test)\n",
    "print(f\"Completed testing model - Accuracy : {accuracy}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
